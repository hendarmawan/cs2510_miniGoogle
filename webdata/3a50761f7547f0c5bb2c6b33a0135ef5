<html>
<head>
<title>CS1699: Homework 2</title>
</head>
<body>
<h2>CS1699: Homework 2</h2>
<b>Due:</b> 10/08/2015, 11:59pm</b> 
<br><br>
<b>Instructions: </b> Please provide your written answers (for parts I, II and III) and your code (for parts II and III). Include your image results in your written answers file. Your written answers should be in the form of a PDF or Word document (.doc or .docx). Your code should be written in Matlab. Zip or tar your written answers, image results and .m files and upload the .zip or .tar file on CourseWeb -> CS1699 -> Assignments -> <b>Homework 2</b>. Name the file YourFirstName_YourLastName.zip or YourFirstName_YourLastName.tar. 
<br><br>

<u>Part I: Short Answers (15 points)</u> 
<br>
<ol>
<li>Suppose we form a texture description using textons built from a filter bank of multiple
anisotropic derivative of Gaussian filters at two scales and six orientations (as displayed
below). Is the resulting representation sensitive to orientation, or is it invariant to
orientation? Explain why.</li>
<img src="textons.png"/><br><br>
<li>Consider the figure below. Each small square
denotes an edge point extracted from an image. Say
we are going to use k-means to cluster these points'
positions into <i>k</i>=2 groups. That is, we will run kmeans
where the feature inputs are the (x,y)
coordinates of all the small square points. What is a
likely clustering assignment that would result? Briefly
explain your answer.</li>
<img src="circle.png" width=200/><br><br>
<li>When using the Hough Transform, we often discretize the parameter space to collect
votes in an accumulator array. Alternatively, suppose we maintain a continuous vote
space. Which grouping algorithm (among k-means, mean-shift, or graph-cuts) would be
appropriate to recover the model parameter hypotheses from the continuous vote space?
Briefly explain.</li>
</ol>

<u>Part II: Color Quantization with K-means (30 points)</u> 
<br><br>
<img src="fish.jpg"/><br><br>
For this problem you will write code to quantize
a color space by applying k-means clustering
to the pixels in a given input image, and
experiment with two different color spaces---
RGB and HSV. 
You are welcome to use the built-in Matlab function <font face="courier new">kmeans</font>.
Include each of
the following components in your submission:
<ol type="a">
<li>
[5pts] Given an RGB image, quantize the 3-dimensional RGB space, and map each pixel
in the input image to its nearest k-means center. That is, replace the RGB value at each
pixel with its nearest cluster's average RGB value. For example, if you set <i>k</i>=2, you might get:
<br><img src="quantized_rgb_k2.png" width=400/><br>
Since these average RGB values may not be integers, you should round them to the nearest integer (1 through 255). Use the following form:<br>
<font face="courier new">function [outputImg, meanColors, clusterIds] = quantizeRGB(origImg, k)</font><br>
where <font face="courier new">origImg</font> and <font face="courier new">outputImg</font> are RGB images of type <font face="courier new">uint8</font>, <font face="courier new">k</font> specifies the number of colors to
quantize to, and <font face="courier new">meanColors</font> is a <i>k</i>x3 array of the <i>k</i> centers (one value for each cluster and each color channel). <font face="courier new">clusterIds</font> is a <i>numpixels</i>x1 matrix (with <i>numpixels = numrows * numcolumns</i>) that says which cluster each pixel belongs to.
Matlab tip: if the
variable <font face="courier new">im</font> is a 3d matrix containing a color image with <i>numpixels</i> pixels, <font face="courier new">X =
reshape(im, numpixels, 3);</font> will yield a matrix with the RGB features as its rows.
</li>
<li>
[5 pts] Given an RGB image, convert to HSV, and quantize the 1-dimensional Hue space.
Map each pixel in the input image to its nearest quantized Hue value, while keeping its
Saturation and Value channels the same as the input. Convert the quantized output back
to RGB color space. Use the following form:<br>
<font face="courier new">function [outputImg, meanHues, clusterIds] = quantizeHSV(origImg, k)</font><br>
where <font face="courier new">origImg</font> and <font face="courier new">outputImg</font> are RGB images of type <font face="courier new">uint8</font>, <font face="courier new">k</font> specifies the number of clusters,
<font face="courier new">meanHues</font> is a <i>k</i>x1 vector of the hue centers, and <font face="courier new">clusterIds</font> is defined as above.
</li>
<li>
[5 pts] Write a function to compute the sum-of-squared-differences (SSD) error between the original RGB pixel values
and the quantized values, with the following form:<br>
<font face="courier new">function [error] = computeQuantizationError(origImg,
quantizedImg)</font><br>
where <font face="courier new">origImg</font> and <font face="courier new">quantizedImg</font> are both RGB images of type <font face="courier new">uint8</font>, and <font face="courier new">error</font> is a scalar
giving the total SSD error across the image.
</li>
<li>
[5 pts] Given an image, compute and display (using the Matlab function <font face="courier new">histogram</font>) two histograms of its hue values. Let the
first histogram use equally-spaced bins (uniformly dividing up the hue values), and let the
second histogram use bins defined by the <i>k</i> cluster center memberships (i.e., all pixels
belonging to hue cluster <i>i</i> go to the <i>i</i>-th bin, for <i>i</i>=1, ..., k). Reuse (call) functions you've written above whenever possible. Use the following form:<br>
<font face="courier new">function [histEqual, histClustered] = getHueHists(im, k)</font><br>
where <font face="courier new">im</font> is the input color image of type <font face="courier new">uint8</font>, and <font face="courier new">histEqual</font> and <font face="courier new">histClustered</font> are the two output
histograms.
</li>
<li>
[5 pts] Write a script <font face="courier new">colorQuantizeMain.m</font> that calls all the above functions
appropriately using the provided image <font face="courier new"><a href="fish.jpg">fish.jpg</a></font>, and displays the results. Include the image results, histograms, and error scores for both the RGB and HSV quantizations. Illustrate the
quantization with at least three different values of <i>k</i>. Be sure to convert an HSV image back
to RGB before displaying with <font face="courier new">imshow</font>. Label all plots clearly with titles. Save your image results and include the results in your written answer sheet.
</li>
<li>
[5 pts] Briefly answer the following. How and why do the results differ based on the value of <i>k</i>? How do the two forms of histogram differ?
How do results vary depending on the color space?  
</li>
</ol>

Useful Matlab functions: <font face="courier new">kmeans, rgb2hsv, hsv2rgb, imshow, double, uint8, reshape, repmat, 
title, histogram</font>.
<br><br>

<u>Part III: Feature Extraction and Description (55 points)</u> 
<br><br>
In this problem, you will implement a feature extraction/detection and description pipeline, followed by a simple image retrieval task. While you will not exactly implement it, <a href="http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf">the SIFT paper</a> by David Lowe is a useful resource, in addition to Section 4.1 of the Szeliski textbook. What you should include in your submission:
<ol type="a">
<li>[15 points] <font face="courier new">function [x, y, scores, Gx, Gy] = extract_keypoints(image)</font> -- Code to perform keypoint detection (feature extraction) using the Harris corner detector, <i>as described in class</i>. 
<ul>
<li>You can use a window function of your choice; opt for the simplest one. <font face="courier new">image</font> is a color image of type <font face="courier new">uint8</font> which you should convert to grayscale and <font face="courier new">double</font> in your function.</li>
<li>Each of <font face="courier new">x, y</font> is an <i>n</i>x1 vector that denotes the x and y locations, respectively, of each of the <i>n</i> detected keypoints. Keep in mind that <i>x</i> denotes the horizontal direction, hence <i>columns</i> of the image, and <i>y</i> denotes the vertical direction, hence <i>rows</i>, but you can count from the top-left of the image. </li>
<li><font face="courier new">scores</font> is an <i>n</i>x1 vector that contains the value to which you applied a threshold, for each detected keypoint. </li>
<li><font face="courier new">Gx, Gy</font> are matrices with the same number of rows and columns as your input image, and store the gradients in the x and y directions at each pixel. </li>
<li>You should also perform non-maximum suppression by only keeping those keypoints whose <i>R</i> score is larger than all of their 8 neighbors; if a keypoint does not have 8 neighbors, do not keep it. Don't remove indices while looping over pixels; instead keep a vector of indices you want to remove (start it empty and concatenate indices to it as needed) and then run the <font face="courier new">unique</font> operation on it, then set the keypoints at those indices to <font face="courier new">[]</font>.</li>
<li>Also output in your function the fraction of keypoints out of the total number of pixels in the image. </li>
<li>You can set the threshold for the "cornerness" score <i>R</i> however you like; for example, you can set it to 5 times the average <i>R</i> score. <font color="red">Or, you can simply output the top <i>n</i> keypoints (e.g. top 1%).</font> </li>
<li><font color="red">The scores/x/y that you output should correspond to the final set of keypoints, after non-max suppression.</font></li>
</ul>
<br>
<!--You should experiment with different values of your threshold, and pick the one that gives the best results. To do this, pick a set of 10 images which you will call your <i>validation set</i>, and tune your threshold on that dataset. Describe your process and observations for picking the keypoint extraction threshold.--> </li>
<li>[15 points] <font face="courier new">function [features, x, y, scores] = compute_features(image, x, y, scores, Gx, Gy)</font> -- Code to perform feature description, similarly to Lowe's paper. 
<ul>
<li><font face="courier new">image, x, y, scores, Gx, Gy</font> are defined as above, but you do not have to convert the image in any way. </li>
<li><face="courier new">features</font> is an <i>n</i>x<i>d</i> matrix, each row of which contains the <i>d</i>-dimensional descriptor for the <i>n</i>-th keypoint. </li>
<li><i>d</i> should be equal to 4x4x8, and contain the 8-dimensional histogram of gradients in each cell of the 4x4 grid centered around each detected keypoint. Each of the 4x4 grid cells has an area of 4x4 pixels, so it contains a summary of the gradients in a 4x4-pixel region. Quantize the gradient orientations in 8 bins (so put values between 0 and <font color="red">22.5</font> degrees in one bin, the <font color="red">22.5 to 45</font> degree angles in another bin, etc.). 
<!--Treat <i>&theta;</i> and <i>180+&theta;</i> as the same angle. -->
To populate the histogram, sum the gradient magnitudes that you have along each of the 8 orientations. Finally, you should clip all values to 0.2 as discussed in class, and normalize each descriptor to be of unit length. You do not have to implement any more sophisticated detail from the Lowe paper. </li>
<!--If any of your detected keypoints are less than 8 pixels from the side of the image, you have to deal with this case; just ensure that the "missing" pixels do not contribute to the histogram. -->
<li>If any of your detected keypoints are less than 7 pixels from the top/left or 8 pixels from the bottom/right of the image, erase this keypoint from the <font face="courier new">x, y, scores</font> vectors and do not compute a descriptor for it.</li>
<li>Since a 16x16 patch of pixels does not exactly center on a pixel, we'll "center" by putting the keypoint pixel at location (8, 8) in the 16x16 patch, i.e. it will technically be upper-left of the absolute center.</li>
<li>To compute the gradient magnitude <i>m(x, y)</i> and gradient angle <i>&theta;(x, y)</i> at point (x, y), take <i>L</i> to be the image and use the formula below, but note that Matlab's <font face="courier new">atan</font> returns values in radians: </li>
</ul>
<table border=0><tr><td width=50></td><td><img src="gradients.png"></td></tr></table></li>
<br>
<li>[10 points] Now pick a <i>test set</i> of 10 images and run your feature extraction and description on them. 
<!--<i>Note that in computer vision, we never tune our parameters on the test set.</i> -->
Visualize the keypoints you have detected, for example by drawing circles over them. Use the <font face="courier new">scores</font> variable and make keypoints with higher scores correspond to larger circles. Note that Matlab's <font face="courier new">plot</font> counts from the top-left when plotting over an image. Save your code in a script called <font face="courier new">part3_c.m</font>. Save the figures that show your features and include them in your answer sheet.</li>
<br>
<li>[15 points] For one of the images in your test set (which we shall call the <i>query</i> image), rank the images in the remainder of the test set based on how similar they are to the query. 
<ul>
<li>For each image in your test set, pick a random subset (see <font face="courier new">randperm</font>) with size that is equal to the smaller of (1) the total number of keypoints detected for that image, and (2) 500 (use <font face="courier new">min(a, b)</font>). </li>
<li>To compute similarity between two images, we will use these <i>subsets</i> of keypoints for the two images. Consider all pairs of keypoints such that the first keypoint comes from the first image, and the second keypoint comes from the second image. Then compute the <i>Euclidean distance</i> between the descriptors of each pair of keypoints. Finally, to get the similarity between the two images considered, average all the Euclidean distances for their keypoint descriptors. Remember that low distance means high similarity. </li>
<li>Show the query image and the remaining images, ranked in descending order of similarity to the query image, in your answer sheet. </li>
<li>Save your code in a script called <font face="courier new">part3_d.m</font>. </li>
<li>Hints: you can use two <font face="courier new"><a href="http://www.mathworks.com/help/matlab/ref/cell.html">cell</a></font> arrays to store (1) the image filenames, and (2) the features of the i-th image, and loop over the length of those (equally-sized) arrays. To compute Euclidean distance between two feature sets, use <br><font face="courier new">sqrt(sum(power(feats1(m, :) - feats2(n, :), 2))</font>.</i> </li>
</ul>
</ol>
Now you have implemented a full basic image retrieval pipeline!
<br>
<br>
<b>Acknowledgement:</b> Parts I and Part II and adapted from Kristen Grauman. Part III was inspired in part by an assignment by James Hays.

<br><br>

</body>
</html>