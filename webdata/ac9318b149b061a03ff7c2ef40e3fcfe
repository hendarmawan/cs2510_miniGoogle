<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />

<title>CS1675</title>
<link rel="stylesheet" href="https://www.cs.pitt.edu/~hwa/cs1675/styles-site.css" type="text/css" />
</head>

<body>
<div id="banner">
<h1><a href="https://www.cs.pitt.edu/~hwa/cs1675/" accesskey="1">CS1675</a></h1>
<span class="description">Introduction to Machine Learning</span>
</div>

<div id="content">
<div class="blog">
<div class="blogbody">

<h3 class="title">Course Information</h3><br/>

Time and place: &nbsp; TTh 11:00-12:15pm (Term 2154), SENSQ 5129<br/>
Prerequisite:   &nbsp; CS 1501 Stat 1000, or by
the instructor's permission.<br/><br/>

<h3 class="title">Suggested Reference Textbooks</h3><br/>
<p>The following books will be on reserve at the Hillman Library.

<ul>
<li>Tom Mitchell. 1997. <a
href="http://www.amazon.com/exec/obidos/ASIN/0070428077/qid=1102634730/sr=2-1/ref=pd_ka_b_2_1/103-8062631-9951031">Machine
Learning</a>. McGraw-Hill.<br/>
<img align=top src="http://images.amazon.com/images/P/0070428077.01.THUMBZZZ.jpg">

<li>Christopher Bishop. 2006.
<a href="http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738/ref=pd_bbs_sr_1?ie=UTF8&s=books&qid=1199566052&sr=8-1">
Pattern Recognition and Machine Learning</a>. Springer.<br/>
<img align=top src="http://ecx.images-amazon.com/images/I/612j5Uo43eL._SL500_PIsitb-sticker-arrow-big,TopRight,35,-73_OU01_SS75_.jpg">

<li>Kevin Murphy. 2012.
<a href="http://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020">Machine Learning: A Probabilistic Perspective</a>. MIT Press.<br/>
<img align=top src="http://ecx.images-amazon.com/images/I/41IsY16f9PL._SX75_CR,0,0,75,75_.jpg">

<li>Stuart Russell and Peter Norvig. 2010 <a href="http://www.amazon.com/Artificial-Intelligence-Modern-Approach-ebook/dp/B004O4BZ16">Artificial Intelligence: A Modern Approach</a>. Prentice Hall. <br/>
<img align=top
src="http://ecx.images-amazon.com/images/I/51bi4EnYE1L._SL500_PIsitb-sticker-arrow-big,TopRight,35,-73_OU01_SS75_.jpg"> 

<li>Trevor Hastie, Robert Tibshirani, Jerome Friedman.  2009. <a
href="http://www.amazon.com/Elements-Statistical-Learning-Prediction-Statistics/dp/0387848576">The
Elements of Statistical Learning (Second Ed.)</a>.  Springer-Verlag.<br/>
<img align=top src="http://ecx.images-amazon.com/images/I/412l0OvYqwL._SL500_PIsitb-sticker-arrow-big,TopRight,35,-73_OU01_SS75_.jpg">

<li>Richard O. Duda, Peter E. Hart, David G. Stork. 2000.
<a href="http://www.amazon.com/exec/obidos/tg/detail/-/0471056693/qid=1102635194/sr=1-1/ref=sr_1_1/103-8062631-9951031?v=glance&s=books">Pattern
Classification.</a>  2nd ed.  Wiley-Interscience.<br/>
<img align=top src="http://images.amazon.com/images/P/0471056693.01.THUMBZZZ.jpg">
</ul>

<br/><br/>

<h3 class="title">Instructor</h3><br/>

<a href="http://www.cs.pitt.edu/~hwa/">Rebecca Hwa</a>, SENSQ 5103<br/>
Email: &nbsp; hwa at cs<br/>
Office Hours: TTh 12:30-1:30pm or by appointment<br/><br/>

<h3 class="title">Teaching Assistant (Grading)</h3><br/>
Chngsheng Liu<br/>
Email: changsheng at cs  </br/>

</div>
</div>

<div class="blog">
<h3 class="title">Class Announcements</h3><br/>

<h2 class="date">Apr. 17, 2015</h2>
<div class="blogbody">
<h3 class="title">Final Review Worksheet</h3>
<p><b>4/22 edit</b>: Here are some sample solutions from folks in the class:
<a href="https://docs.google.com/document/d/1X5pf0qbTVZP56l8hWI0mykEJ71sxW5_FUUcPu2CjCeY/edit?usp=sharing">J. Gelpi and R. Wei</a>, <a href="https://docs.google.com/document/d/1oEaQ3Q57-GOjq0saUOYR1LAEkSH0VJwNOuQ3EONIM9w/edit?usp=sharing">M. Schnur</a>. Also, for sample solutions to problems in homework 4, please see the final versions of the review notes for Lectures 16, 17/18.

<p> Here is <a href="
https://docs.google.com/document/d/1ud-Lz6M011W11pTTtyukId3u0uzRD0uJM6AsTOQFlyA/edit?usp=sharing">the link</a> to the final review worksheet. If you'd like to work on this and submit your solution for others to study off of, please do so in such a way that I can post your (checked) solutions by Wednesday night. 

<p>The same also holds for the review notes. If you would like to prepare a lecture review notes, please let me know as soon as possible. Again, we aim to have all lecture review notes posted by Wednesday night.
</div>
<h2 class="date">Apr. 14, 2015</h2>
<div class="blogbody">
<h3 class="title">Info about the Project Report</h3>
<p> Each team must turn in one report. You should email the report to me by
<b>8:00am, 4/25/2015.</b> I will email back a confirmation of receipt
by noon that day. If you did not hear from me, it meant that I didn't get your
report, and you should contact me as soon as possible.

<p>The report should be well-organized and well-written (some general
guidelines are posted below). It should convey clearly and precisely
what your team has done. <i>Do not submit a long report just for the
length's sake.</i> For your target reader, don't picture only me, but
also another team of students taking this class in the future, trying
to reproduce and/or extend your work. 

<p>Here is a suggested outline for the report. Note that the page
length for each section is just a suggestion: you can go over or under
as your situation calls for.  
<ul>
<li>Introduction -- This should take up about 1 to 1.5 pages. It should clearly lay out the general problem domain as well as the specific problem(s) you are trying to address. It should also briefly summarize your approach, experiment, and findings. Basically, a reader should be able to get an overall sense of what the paper is about from reading the introduction along.

<li>Background -- This should take up about 1 to 2 pages. In this section, you elaborate on the problem domain (especially if it requires specialized knowledge). You can also use this section to describe relevant previous work. <i>Do not just give a summary of what they did, however. You should discuss how their work relates to yours</i>.

<li>Approach (Method) -- This should take up about 2-3 pages. This is where you describe your algorithm/system. If appropriate, you should identify where the machine learning component fits within the larger framework. In any case, you should thoroughly discuss the machine learning part -- i.e., you should cover all the steps of the machine learning design cycle.

<li>Experimental Setup -- This should take up about 1 page. In this section you provide more specific information about the experiment you conducted: What experiment will you perform? How are the data collected? What is the evaluation metric?

<li>Experimental Results and Discussions -- This should take up at least 2-3  pages (it can be longer). For each experiment you performed, talk about the goal of the experiment (in terms of the problem you are trying to address), what you expected to find, what the results actually showed, what are the implications of these results. Include relevant tables and graphs -- be sure to explain how to read these tables and graphs if you do include them.

<li>Conclusion and Future Work -- This should be at most a page. Here, you briefly summarize what we've learned (e.g., to what extent was your approach successful at addressing the problem you wanted to tackle). You then talk about what you might do different or what you might do next.
</ul>

<h3>Individual Questionnaire on the Team Project Experience</h3>
<p> Please let me know how your team experience went by answering the
following questions and email it to me directly. 
<ul>
<li> How frequently did your team communicate with each other? Did you mostly meet in person or did you use IM/Skype/email/etc.?
<li> How were the project tasks divvied up? Do you think everyone had a fair share of work?
<li> What obstacles did the project run into? How did you, as a team, resolve them? Do you think the obstacles were resolved in a satisfactory way?
<li> If you ever find yourself in the same class or company as your teammates, would you work with them again?   
</ul>


</div>

<h2 class="date">Apr. 9, 2015</h2>
<div class="blogbody">
<h3 class="title">Lecture 24</h3>
<p> Here is the final version of the <a href="https://docs.google.com/document/d/157eZMENxcJGtxvaDst47doQAtaAOU_DJDt-pr_8ceuM/edit?usp=sharing">review notes</a>: Thanks to Zach Alcorn for this contribution.

<p> This lecture focused on unsupervised learning. Here is the <a href="https://docs.google.com/document/d/1sUTImOigUal41lcxGaVpPpXecdv63mgvSlSEO7eut0o/edit?usp=sharing">skeletal lecture review notes</a>.

<h3 class="title">Info on Project Presentation</h3>
<p> Each team will have 15 minutes for their presentation (including
  set-up time and question time). Because our schedule is tight, time
  limit will be strictly enforced. I recommend preparing a set of ten
  slides, spending about a minute per slide. That will leave you some
  time for questions from the audience. You can decide amongst yourselves
  whether everyone will take turn speaking. 

<p> The content of your presentation should go something like this:
<ul>
<li> <b>Introduction [<1 min]:</b> Give some general background on the domain of your problem -- since you've already done this during our initial progress update,
 you don't have to spend a lot of time on this part. 
<li> <b>Problem statement[1-2 min]:</b> Make a precise statement about what it
  is you are solving. Highlight the challenges of this problem (from
  the perspective of machine learning). 
<li> <b>Approach[3-4 min]:</b> How did you try to solve the problem? You should
  be clear about how the data is mapped into input instances in your
  machine learning framework, and you should be clear about how the
  output from your system is used to solve the problem. (For some
  projects, this may be obvious, and you can get through the
  explanation quickly.) [<i>optional</i> <b>Related work:</b>] If other people have looked at similar problems as yours, very briefly talk about what they've done and how they relate to yours.
<li> <b> Experimental Setup[1 min]:</b> Describe how you will validate your
  approach through an experiment. Present the details of the
  experimental setup (how do you prepare the train/test split(s); what
  are you using for an evaluation metric (justify why it's a good
  metric if non-obvious); what is your baseline comparison.
<li> <b> Results[1-2 min]:</b> It is OK if you don't have complete results yet
  by the week of the presentation. However, you should be able to make
  a template of the eventual result table. You can present a partially
  filled table if you have some initial results. Even without concrete
  numbers, you can discuss possible outcomes hypothetically (Suppose
  you are comparing two methods: A and B; what is the implication on
  your problem if Method A outperforms method B?  What if B
  outperforms A? What if the difference is not statistically
  significant?).
<li> <b>Summary/Future Work[2-3 min]:</b> Make a brief summary statement about
  your project. Highlight interesting things you've learned about the
  problem (either in terms of the challenges or in terms of the
  approach). Were there things you initially thought were important
  but weren't? Anything else that surprised you? If you have more
  time (another semester, say), what would you do to extend the
  project? 
</ul>

<p> <b>Grading:</b> You are evaluated on not only your team's
  presentation, but also individually on your participation of the
  presentation week: you should attend the class even when your team
  is not presenting; you are encouraged to ask questions about 
  the other teams' projects.

<p><b>Schedule:</b> 
<pre> 
<i>Tuesday, April 14</i>
   Drobitch,Lowe,Stavish,Saylor
   Hoddinott, Hron Weigle, Kuhn, Strouse
   Budd, Gelpi, Wei
   Croul, Kerestan, Mauro
   Mulky, McDonald, Zivanovich
<i>Thursday, April 16</i>
   Ferrel, Mash, Peluso
   Hachten, Liu, Zhang
   Engler, Minkus, Roggeman 
   Dubnik, Ghelani, Lin
   Alcorn, Dryslewski, Schnur
</pre>
</div>

<h2 class="date">Apr. 7, 2015</h2>
<div class="blogbody">
<h3 class="title">Lecture 23</h3>
<p> We completed our discussions on supervised learning with Hidden Markov Model and Naive Bayes. Here is the <a href="https://docs.google.com/document/d/1tvyArLp3g5ovMAq-00hkezxAbMRtECJYAvHeSePijTk/edit?usp=sharing">skeletal review</a> for today's lecture (has some overlap with the previous set of notes.).
</div>

<h2 class="date">Apr. 1, 2015</h2>
<div class="blogbody">
<h3 class="title">April Fools: ML Edition</h3>
<p> Thanks to Coleman Stavish for the pointer: http://www.oneweirdkerneltrick.com
</div>

<h2 class="date">Mar. 31 and Apr. 2, 2015</h2>
<div class="blogbody">
<h3 class="title">Lecture 21, 22</h3>
<p> We talked about a probabilistic learning model called the Naive Bayes Model. Here is a <a href="https://docs.google.com/document/d/1_SCll3Ir2mRTodxRW6F3ux-DlVAIKY5XQLqS7HW3-d0/edit?usp=sharing">review skeleton</a> for the lecture.

<h3 class="title">Homework 4</h3>
<p> This assignment is a problem set; it's due by 4/7 11:59PM. Here are the <a href="http://www.cs.pitt.edu/~hwa/cs1675/hw4.pdf">details</a>.
</div>

<h2 class="date">Mar. 26, 2015</h2>
<div class="blogbody">
<h3 class="title">Lecture 20</h3>
<p> This lecture introduces a probabilistic interpretation for concept learning, which we talked about a long time ago at the start of the semester. <a href="https://docs.google.com/document/d/1ZOWQdXQIE3q6oYTSrvSRmemAynaCkYsh-3fJYDJXLbQ/edit?usp=sharing">Here</a> is the skeletal review notes for this lecture.
</div>

<h2 class="date">Mar. 24, 2015</h2>
<div class="blogbody">
<h3 class="title">Lecture 19</h3>
<p> This lecture concludes our discussion on SVM. <a href="https://docs.google.com/document/d/1tBxtVOU9JPBNH0HRC8Nr4urMqp9N_QekvUWkyVPdnJY/edit?usp=sharing">Here</a> is a skeleton of the review notes for the kernel function portion.
</div>

<h2 class="date">Mar. 17-19 2015</h2>
<div class="blogbody">
<h3 class="title">Lecture 17, 18</h3>
<p> Here is the final <a href="http://www.cs.pitt.edu/~hwa/cs1675/03.17.pdf">review notes</a> (including a sample solution for the SVM related questions in homework 4). Thanks to Sarah Dubnik for the preparation.
<p> This week we've introduced one more model for binary classification
  called the support vector machine (SVM). Here is this week's <a href="https://docs.google.com/document/d/1x1Um31Yh1ng-GQeusFbf4k-IS29ymJJnbxkgMI0wnao/edit?usp=sharing">review skeleton</a>. 
</div>

<h2 class="date">Mar. 5, 2015</h2>
<div class="blogbody">
<h3 class="title">Lecture 16</h3>
<p> Here is the <a href="http://www.cs.pitt.edu/~hwa/cs1675/03.05.pdf">final version</a> of the review notes, including a sample solution for the neural network related questions in homework 4. Thanks to Nick Drysleuski for the preparation.

<p> In this class, we went over the training algorithm for
  multi-layered neural network, called backward propagation, or
  <i>backprop</i> for short. It is an instance of gradient descent.  
Here is today's <a href="https://docs.google.com/document/d/1RRS5NpfvPqGDFdctXTYV6lQFAAsITqipsiqYOY0YFik/edit?usp=sharing">review skeleton</a>. 
</div>

<h2 class="date">Mar. 3, 2015</h2>
<div class="blogbody">
<h3 class="title">Post-Lecture Review for Lecture 15</h3>
<p>Here is the final version
  of <a href="http://www.cs.pitt.edu/~hwa/cs1675/03.03.pdf">the
    post-lecture review notes</a>. Many thanks to Peter Mash for preparing the notes.


<h3 class="title">Project Proposal</h3>
<p> Each team should send me a project proposal by email by 11:59pm, this Friday. Your project proposal does not have to be exhaustive, but it should contain the following information:
<ul>
<li> Team members
<li> Problem domain (e.g., "Given a user and a city, propose 10 restaurants that the user might like.")
<li> Data collection: where you think your data will come from. Talk about its form and how much preparation you may need to perform on it. (e.g., "we will use the publicly available Yelp dataset. We plan to make each review a different instance of a labeled example. We will need to do the following text processing tasks: X, Y, Z... By the end of the processing step, we will have a collection of individual reviews and their star ratings.")
<li> Feature choice: similar to homework 1. If you are using an existing feature set, talk about how you will make changes (i.e., create more combination of features, removing irrelevant features)
<li> Model choice/training: talk about how you plan to build your final system out of known models (this is especially important if your main problem is not an obvious classification or regression problem -- e.g., "we will train a classifier that predicts how much a user might like given restaurant based on the set of features discussed above. We will run the classifier on all the restaurants in the city and then sort the outputs by the classifier's confidence. We will compare several neural network architectures, including a single layer network (i.e., logistic regression) as a baseline. We will use standard gradient descent training methods to train the neural networks. Because we are designing our own network architecture, we cannot use standard packages such as Weka. Instead, we will use a Python library, called "PyBrain" to help with the training.").
<li> Evaluation: talk about how you plan to evaluate your final system. 
</ul>

<h3 class="title">Lecture 15</h3>
<p> In this class, we've introduced a non-linear model called a neural network. Here is today's <a href="https://docs.google.com/document/d/1nfY4rDpk11iaEAqTO-vqtVKgl6SF9yjj1kcuOZR9Brg/edit?usp=sharing">review skeleton</a>. Thanks to Peter Mash for the review.
</div>

<h2 class="date">Feb. 24, 2015</h2>
<div class="blogbody">
<h3 class="title">Midterm Review</h3>
<p> Today during class, we did a brief review of the course topics
  covered thus far. Also, if you are working on the review questions
  and want to compare answers, here is a set
  of <a href="https://docs.google.com/document/d/11aMdIW4jjr6bBlaMpRjXUJ3azfdJGCPR3JL1NnpmGbI/edit?usp=sharing">sample
  solutions</a> made by one of the students in the class. (Many thanks
  to Joey Gelpi for volunteering.) Note: if you have questions or if
  you find any calculation errors, please use the "comment" feature to
  let us know.

</div>

<h2 class="date">Feb. 19, 2015</h2>
<div class="blogbody">
<h3 class="title">Post-Lecture Review for Lecture 14</h3>
<p>Here is the final version of <a href="http://www.cs.pitt.edu/~hwa/cs1675/02.19.pdf">the post-lecture review notes</a>. Many thanks to Sarah Dubnik for preparing the notes.

<h3 class="title">Lecture 14</h3>
<p> Today's lecture is about overfitting and underfitting for linear models (like linear regression). Here is a <a href="https://docs.google.com/document/d/1OGB68ApOVusdWiD2xqqdJNvGZsofV3U8-7y1jH_DUcg/edit?usp=sharing">skeleton</a> for today's post-lecture review. Thanks to Sarah Dubnik for volunteering.

</div>

<h2 class="date">Feb. 18, 2015</h2>
<div class="blogbody">
<h3 class="title">Midterm Information</h3>
<p> Our midterm is set for next Thursday, Feb. 26th. It will cover everything we've talked about so far up through tomorrow's lecture. <a href="https://docs.google.com/document/d/1ED6m9J2cX93hXnBZM6qmJLTKh68q0Vph9o3psz3loYM/edit?usp=sharing">Here</a> are some additional information about the test as well as some sample questions.
</div>

<h2 class="date">Feb. 17, 2015</h2>
<div class="blogbody">
<h3 class="title">Lecture 13 </h3>
<p> Today we talked about adapting linear binary classifiers for
  multiclass classification. Here is the <a href="https://docs.google.com/document/d/1UavLm1DPUjmoNkzNZ8zeiQfOlaYiiIm9SNlVYUT3MLQ/edit?usp=sharing">
  skeleton</a> for the post-lecture review.
</div>

<h2 class="date">Feb. 16, 2015</h2>
<div class="blogbody">
<h3 class="title">Team Assignment for Term Project </h3>
<p> I've gone through all your questionnaire answers. Based on your
  common interests and experiences, I've come up with 
  the following team assignments. We will spend a little time tomorrow
  in class for you to introduce yourselves to your teammates; if you
  cannot make it to class tomorrow, please try to do so over email
  (you can use find.pitt.edu if you don't know your teammates' email addresses).
<pre>
Alcorn,Zachary
Dryslewski,Nicholas
Schnur,Matthew

Budd,Jonathan
Gelpi,Joseph
Wei,Zhiyao

Croul,Andrew
Kerestan,Benjamin
Mauro,Dean

Drobitch,Jon
Lowe,Philip
Stavish,Coleman
Saylor, Steven

Dubnik,Sarah
Ghelani,Samit
Lin,Yiran

Engler,William
Minkus,Kevin
Roggeman,Joel

Ferrel,Thomas
Mash,Pete
Peluso, Nicholas

Hachten,Nathaniel
Liu,Xin
Zhang,Gong

Hoddinott,Ethan
Strouse,Cullen
Hron Weigle,Charles
Kuhn,Michael

Mulky,Virginia
McDonald,Ryan
Zivanovich,Sheridan
</pre>
</div>

<h2 class="date">Feb. 12, 2015</h2>
<div class="blogbody">
<h3 class="title">Midterm Announcement</h3>
<p> The midterm is set for 2/26 (Thursday). It will cover everything we've talked about so far through next Thursday (2/19).

<h3 class="title">Lecture 12</h3>
<p> Today we introduced logistic regression, another linear model for classification. Here is the post-lecture review <a href="https://docs.google.com/document/d/1tJ-ZYgzSn7Tt2Vg2sZsbYPgUYYfdWPECt1hFDlm-L64/edit?usp=sharing">skeleton</a>. Thanks to Nathaniel Hachten for volunteering.
</div>

<h2 class="date">Feb 10, 2015</h2>
<div class="blogbody">
<h3 class="title">Homework 3</h3>
<p> This is due Tuesday 2/24. The details of the assignment is <a href="http://www.cs.pitt.edu/~hwa/cs1675/hw3.pdf">here</a>. If you need me to help you find a working implementation of ID3, you should let me know by the morning of Thursday 2/12; I will try to find one for you by Friday.

<h3 class="title">Finalized Post-Lecture Review</h3>
<p> <b>2/18</b>: Here is the finalized post-lecture <a href="http://www.cs.pitt.edu/~hwa/cs1675/02.10.pdf">review notes</a> for lecture 11. Many thanks to Will Engler for contributing.

<h3 class="title">Lecture 11</h3>
<p> This week we are focusing on applying linear models for classification problems. Here is today's <a href="https://docs.google.com/document/d/1_hBuxZXqAojuU1DSkU0XdLJQW-gjIaYyoPmtfnkcV2M/edit">review skeleton</a>. Thanks to Will Engler for volunteering to review the notes.

</div>

<h2 class="date">Feb 5, 2015</h2>
<div class="blogbody">
<h3 class="title">Team Project Initial Questionnaire</h3> 

<p> Please fill out the
  following <a href="http://goo.gl/forms/axtPTXzlLg">form</a> by 
  the morning of February 12th. This will help me with assigning
  people to teams for the term project. If there are people with whom
  you'd like to be teammates with, I'll do my best to fullfill the
  request, but I cannot guarantee it.

<h3 class="title">Homework 2 Sample Output</h3>
<p> If you were unable to complete Homework 2 by the submission deadline, or if you were unsure about your implementation, please use these few days to revise your implementation of ID3 prior to the announcement of Homework 3. 

In case if it's helpful, here is ID3's output tree for the sample data. Because this dataset is so small and simple, all three diversity functions resulted in the same tree; this tree performs with 100% accuracy on the training examples as well as the two test examples.
<pre>
Outlook=sunny
  Temp=hot no : 3/3
  Temp=mild
    Humidity=normal yes : 1/1
    Humidity=high no : 1/1
  Temp=cold yes : 1/1
Outlook=overcast yes : 5/5
Outlook=rain no : 5/5
</pre>

<h3 class="title">Lecture 10 post-lecture review skeleton </h3>
<p> Here is the 
<a href="https://docs.google.com/document/d/1Lg05FBXnWeluIyuGgPwMmKRz4FlEKHBK4WMketVOkJ0/edit?usp=sharing">
link</a> to today's skeletal notes. If you have
  questions for the reviewer, please use the comment feature. 
<p>
</div>

<h2 class="date">Feb 3, 2015</h2>
<div class="blogbody">
<h3 class="title">Finalized Post-Lecture Review</h3>
<p> <b>2/9</b>: Here is the finalized post-lecture <a href="http://www.cs.pitt.edu/~hwa/cs1675/02.03.pdf">review notes</a> for lecture 9. Many thanks to Jon Budd for contributing.

<h3 class="title">Team Project Ideas</h3>
<p> Several people have submitted some interesting learning problems
  for the bonus portion of Homework 1. I would like to post them for
  the class to see and discuss. Even if they don't become any team's
  term project, your reactions to these problems will help me decide
  how to group you together with like-minded people.

  <p>If you've submitted a bonus answer for homework 1, but would like
  me to not include an excerpt of your writeup, or if you just don't
  want your name to be made public, please email me by 8am 2/4
  Wednesday.

<h3 class="title">Lecture 9 Errata and post-lecture review skeleton </h3>
<p> I realized after class that I dropped a crucial set of negative
  signs with respect to the delta w's for the gradient descent algorithm. I've made the changes in red in
  the current skeletal review notes. Please update your own notes
  accordingly. Here is the 
<a href="https://docs.google.com/document/d/1OCkE5Ajl5eHCYHtzUptc3Vk6PLa6LboZNk6o_OyCxGg/edit?usp=sharing">
link</a> to the skeletal notes. If you have
  questions for the reviewer, please use the comment feature. Thanks to Jon Budd for volunteering.
<p>

</div>


        <h2 class="date">Jan 29, 2015</h2>
	<div class="blogbody">
	<h3 class="title">Lecture 8</h3>
	<p> In this class, we went over some commonly used tests for statistical significance. We focused on the mechanics of doing these tests and not so much on the statistical theories behind these tests. Nonetheless, you should be able to:
	<ul>
	<li> recognize common machine learning experimental scenarios for applying each test
	<li> carry out the test procedure by yourself (i.e., without running a full statistical analysis package; calculators are OK).
	</ul>
	<p> Here are my <a href="http://www.cs.pitt.edu/~hwa/cs1675/01.29.pdf">review notes</a> for this lecture. 

        <h3 class="title">Homework 2</h3>
	<p> The first assignment is due by 11:59pm, 2/5/2015. Here
	are
	the <a href="http://www.cs.pitt.edu/~hwa/cs1675/hw2.pdf">details</a>. Note:
	homework 3 will be building on top of this one.

	<p><b>Edit 1/30</b>: Some people reported having problems opening up the sample.config file. Please replace "www" with "people" in the URL, and that should work. Here is the content of the sample.config file in case if you are still having trouble:
<pre>
yes,no
Outlook,sunny,overcast,rain
Temp,hot,mild,cold
Humidity,normal,high
Wind,weak,strong 
</pre>

	<p><b>Edit 1/31</b>: Someone asked about the output format: for each leaf node, you have to give a fraction (e.g., #A/#total) -- This fraction tells us the diversity of the subset of training examples that reaches the leaf node. In the toy example in Review Notes #6, the left most leaf node (whose path is x1=yes, x2=A, x3=T) would have 1/1: it's associated with one training example (ex#6) which has a - label; in contrast, for the leaf node at the end of path x1=no, x2=A, x3=T, we got the + label because when there were no training examples with those features, we had to back off to x3's training set (ex#4,ex#7) and take the majority vote, which defaults to + when there is a tie. So the fraction associated with this node would be 1/2. For the leaf node at the end of path x1=no, x2=A, x3=F, again, we'd output 1/2 because one of the two examples associated with this leaf node is labeled +.
         </div>

	<h2 class="date">Jan 27, 2015</h2>
	<div class="blogbody">
	<h3 class="title">Lecture 7</h3>
	<p> This week's two lectures are about experimental
	methodologies. We use dealing with overfitting in learning
	decision trees as our running example, but the methodology
	principle applies to other machine learning methods that we'll
	be talking about for the rest of the term. From this lecture,
	you should know what a k-fold cross validation is, and why we
	prefer to conduct experiments in this way.

        <p> Here is
        my <a href="http://www.cs.pitt.edu/~hwa/cs1675/01.27.pdf">post-lecture
        review</a> notes for this lecture. 

	</div>  

	<h2 class="date">Jan 22, 2015</h2>
	<div class="blogbody">
	<h3 class="title">Lecture 6</h3>
	<p> We continued to talk about learning decision trees in this lecture. For the most part, we discussed ways to quantify the importance of a feature. We then saw how the full ID3 algorithm worked on a small toy example. We finished up the lecture with a brief discussion on training over-fitting, which we will resume discussing next time.

	<p> For this lecture, here are some major concepts you should be familiar with:
	<ul>
	<li> The definition of entropy. Know how to calculate it for a distribution. Know how it fits in the context of ID3. You are not required to understand the background on entropy, though it might help you to understand why it's used in learning a decision tree.
	<li> The definition of the Gain function. Know how to calculate it when given a set of training examples and a feature. Know how it fits in the context of ID3.
	<li> The workflow of the ID3 algorithm. Given a set of training examples, you should be able to trace through ID3 to find the resulting decision tree.
	</ul>
	<p> Here is my <a href="http://www.cs.pitt.edu/~hwa/cs1675/01.22.pdf">post-lecture review</a> notes. 
	</div>

	<h2 class="date">Jan 20, 2015</h2>
	<div class="blogbody">
	<h3 class="title">Lecture 5</h3>
	<p> We introduced a new model called decision trees and talked
	about reasonable learning algorithms for it. The major ideas from today's lecture are:
	<ul>
	<li> Model choice: the decision tree representation
	<li> Learning algorithm: ID3 (Iterative Dichotomizer 3)
	</ul>

	<p> Here is today's <a
	href="http://www.cs.pitt.edu/~hwa/cs1675/01.20.pdf">post-lecture
	review</a>.
	</div>

	<h2 class="date">Jan 15, 2015</h2>
	<div class="blogbody">
	<h3 class="title">Lecture 4</h3>
	<p> We continued our discussion from Tuesday. The major ideas from today's lecture are:
	<ul>
	<li> Model choice: representing target concept class as monotonic k-literal disjunctive functions
	<li> Interaction between feature choice and model choice: your
	model choice may require your features to be in a certain form
	(e.g., binary valued). 
	<li> Interaction between model choice and training (learning
	algorithms): There may be multiple learning algorithms that
	operate on the same or similar model space. In addition to
	Winnow, we were introduced briefly to the Perceptron Update
	Algorithm.
	</ul>

	<p> Here is today's <a
	href="http://www.cs.pitt.edu/~hwa/cs1675/01.15.pdf">post-lecture
	review</a>.
	</div>

	<h2 class="date">Jan 13, 2015</h2>
	<div class="blogbody">
	<h3 class="title">Lecture 3</h3>
	<p> The major ideas from today's lecture are:
	<ul>
	<li> The machine learning design cycle (and how it relates to the main questions we need to address to set up a task as a machine learning problem)
	<li> A class of learning problems known as "Concept Learning." (supervised, binary classification)
	<li> The inductive learning hypothesis
	<li> New Model: k-literal disjunctive functions
	<li> New Learning Algorithm: Winnow (variation 1)
	</ul>

	<p> In today's <a
	href="http://www.cs.pitt.edu/~hwa/cs1675/01.13.pdf">post-lecture
	review</a>, there is also an example write-up for homework 1.

	<h3 class="title">Policy Acknowledgement</h3>
	<p> Here is a list of people whom I have heard from. If you
	did send me an email, but are not on the list, please talk to
	me Thursday.
<pre>
Alcorn,Zachary
Budd,Jonathan
Croul,Andrew 
Drobitch, Jonathan
Dryslewski,Nicholas
Dubnik,Sarah
Engler,William
Ferrell,Thomas
Gelpi,Joseph
Ghelani,Samit
Hachten,Nathaniel
Hoddinott,Ethan
Hron Weigle, Charles
Kerestan,Benjamin 
Kuhn,Michael
Kuzniar,John
Lin,Yiran
Liu,Xin
Lowe,Philip 
Mash,Peter
Mauro,Dean
McDonald,Ryan
Minkus,Kevin 
Mulky,Virginia
O'Brien,Matthew
Peluso,Nicholas
Roggeman,Joel
Saylor,Steven
Schnur,Matthew
Stavish,Coleman
Strouse,Cullen
Wei,Zhiyao
Zhang,Gong
Zivanovich,Sheridan
</pre>
	


	</div>

        <h2 class="date">Jan 12, 2015</h2>
	<div class="blogbody">
        <h3 class="title">Homework 1</h3>
	<p> The first assignment is due by 11:59pm, 1/20/2015. Here
	are
	the <a href="http://www.cs.pitt.edu/~hwa/cs1675/hw1.pdf">details</a>.
	Tomorrow's (1/13) lecture will probably be helpful for this
	assignment.
         </div>

        <h2 class="date">Jan 8, 2015</h2>
	<div class="blogbody">
        <h3 class="title">Lecture 2</h3>
        <p> In this lecture we've talked about how to set up a problem using a machine learning approach. In particular,
        <ul>
	<li> In a well-posed machine learning problem, you should be able to clearly state what is the class of tasks at hand (e.g., play checkers); what is the performance measure (e.g., percentage of games won); and what is the training experience (e.g., two computers playing each other).
        <li> There are considerations for deciding on a good performance measure that depend on the tasks at hand.
        <li> There are many ways to set up the training experience. Again, you need to consider the domain and the resources available to you in order to pick a reasonable choice.
        <li> To specify a learning system for a problem (which can be thought of as a function that takes an input and outputs a prediction), you need to further determine:
	<ul>
	<li> What should be the form of the input and output to the function we want to learn?
	<li> What form should the function itself take?
	<li> What algorithm should we use to learn the function from the training experiences?
	</ul>
        </ul>
	<p> For the first few weeks, I'll continue to post examples of post-lecture reviews. Here is <a href="http://www.cs.pitt.edu/~hwa/cs1675/01.08.pdf">one</a> for lecture 2.
        </div>

	<h2 class="date">January 6, 2015</h2>
        <div class="blogbody">
	<h3 class="title">Lecture 1</h3>
	<p> This is an introductory lecture. We have talked about:
        <ul> 
	  <li> What is machine learning
	  <li> What are examples of machine learning applications 
          <li> Why we might want to develop machine learning methods
        </ul>
        <p> Here is an example of
        a <a href="http://www.cs.pitt.edu/~hwa/cs1675/01.06.pdf">post-lecture
        review</a> for today's lecture. 
       </div>

       <h2 class="date">Jan 1, 2015</h2>
        <div class="blogbody">
	<h3 class="title">Preliminaries</h3>
	
	<p>Welcome to CS1675. If you plan to take this class, please read through the course policy and send me an email to acknowledge that you have read and understood the policy. Alternatively, you may give me a hard copy of a signed statement if you wish. <i>NOTE: an assignment folder will not be created for you until I have received the acknowledgement. </i>
	
	</div>
	
<hr>

<h3 class="title">Acknowledgements</h3></br>

<p>Some of the materials used in this course are based on the courses
developed by Hal Daume, Lise Getoor, Milos Hauskrecht, Thorsten Joachims, Michael Littman, Andrew Moore, Andrew Ng, Ray Mooney, Ron Rivest, and CMU courses 10-701 and 15-781.

</div>
</div>
<div id="links">
<div class="side">
<a href="index.html">Home</a><br/>
<a href="policy.htm">Course Policies</a><br/>
<a href="syl.htm">Syllabus</a><br/>
<a href="links.htm">Links</a><br/>
</div>

<br clear="all" />

</body>
</html>
