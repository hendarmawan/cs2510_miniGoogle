<html>
<head>
<title>CS1699: Homework 3</title>
</head>
<body>
<h2>CS1699: Homework 3</h2>
<b>Due:</b> <font color="red">11/03/2015</font>, 11:59pm
<br><br>
<b>Instructions: </b> Please provide your written answers and your code. Include your image results in your written answers file (you don't have to submit them as separate files). Your written answers should be in the form of a <b>single</b> PDF or Word document (.doc or .docx). Your code should be written in Matlab. Zip or tar your written answers and .m files and upload the .zip or .tar file on CourseWeb -> CS1699 -> Assignments -> <b>Homework 3</b>. Name the file YourFirstName_YourLastName.zip or YourFirstName_YourLastName.tar. 
<br><br>

<h3><u>Part I: Circle detection with the Hough Transform (35 points)</u></h3>

<img src="egg.jpg"/>
<br><br>
Implement a Hough Transform circle detector that takes an input
image and a fixed radius, and returns the centers of any detected
circles of about that size. <b>You are not allowed to use any built-in Matlab functions for finding edges or circles!</b> Include the following:
<br>
<ol>
<li>[5 pts] 
<font face="courier new">function [edges] = detectEdges(im, threshold)</font> -- A function to compute edges in an image. <font face="courier new">im</font> is the input image in <font face="courier new">uint8</font> type and in color, and <font face="courier new">threshold</font> is a user-set threshold for detecting edges. <font face="courier new">edges</font> is an <i>N</i>x4 matrix containing 4 numbers for each of <i>N</i> detected edge points: <i>N(i, 1)</i> is the <i>x</i> location of the point, <i>N(i, 2)</i> is the <i>y</i> location of the point, <i>N(i, 3)</i> is the gradient magnitude at the point, and <i>N(i, 4)</i> is the gradient orientation (non-quantized) at the point. 
<ul>
<li>In this function, simply compute the gradient magnitude and orientation at each pixel, and only return those (x, y) locations with magnitude that is higher than the threshold. Use the magnitude and orientation equations (and reuse as much code as possible) from HW2.</li>
<li>Allow for the user to skip passing a value for a threshold; use <font face="courier new">if(nargin==1)</font>, and set a default value for the threshold, e.g. some multiple (try 1 through 5) of the average gradient magnitude in the image.</li>
<li>Remember that the <i>x</i> direction corresponds to columns and the <i>y</i> direction corresponds to rows.</li>
<li>At the end, display, save, and include in your writeup the thresholded edge image for an image of your choice, as in slide 24 <a href="http://people.cs.pitt.edu/~kovashka/cs1699/vision_05_edges.pdf">here</a>.</li>
</ul>
</li>
<br>
<li>[15 pts]
<font face="courier new">function [centers] = detectCircles(im, edges, radius, top_k)</font> -- A function to find and visualize circles from an edge map. 
<font face="courier new">im, edges</font> are defined as above, <font face="courier new">radius</font> specifies the size of circle we are looking for,
<!--and <font face="courier new">usegradient</font> is a flag that allows the user to optionally exploit the gradient direction measured at the edge points.-->
and <font face="courier new">top_k</font> says how many of the top-scoring circle center possibilities to show. 
The output <font face="courier new">centers</font> is an <i>N</i>x2 matrix in which each row lists the <i>x, y</i>
position of a detected circle's center. 
<ul>
<li>Consider using <font color="red"><font face="courier new">ceil(a / quantization_value)</font> and <font face="courier new">ceil(b / quantization_value)</font> (where, for example, <font face="courier new">quantization_value</font> can be set to 5)</font> to easily figure out quantization/bins in Hough space. <font color="red">Don't forget to multiply by <font face="courier new">quantization_value</font> once you've figured out the Hough parameters with most votes.</font></li>
<li>Ignore circle centers outside the image.</li>
<li>Set 1 as the default value for <font face="courier new">top_k</font> (if the user didn't specify this value) using <font face="courier new">nargin</font> as above.</li>
<li>Use this line at the end of your function to visualize circles: <font face="courier new">figure; imshow(im); viscircles(centers, radius * ones(size(centers, 1), 1));</font>.</li>
<li>In your writeup, describe how your code would be different if you didn't know the gradient orientation.</li>
</ul>
</li>
<br>
<li>[10 pts] Demonstrate the function applied to the provided images <font face="courier new"><a href="jupiter.jpg">jupiter.jpg</a></font> and
<font face="courier new"><a href="egg.jpg">egg.jpg</a></font>. Display the images with detected
circle(s), labeling the figure with the radius, save your image outputs, and include them in your writeup. You can use <font face="courier new">impixelinfo</font> to estimate the
radius of interest manually. For each image, include results for at least 5 different radius values.</li>
<br>
<li>[5 pts] For one of the images, demonstrate the impact of the vote space quantization (bin size).</li>
</ol>

Useful Matlab functions: <font face="courier new">ind2sub, ceil, <font color="red">atan</font>, sin, cos, viscircles, impixelinfo</font>.
<!--<br>Matlab tip: note that the row number ("y" value) for an image position is flipped from Cartesian coordinates, increasing as we move down.-->
<br><br>

<h3><u>Part II: Video search with Bags of Visual Words (65 points)</u></h3>

For this problem, you will implement a video search method to retrieve relevant frames from a
video based on the features in a query region selected from some frame. This assignment is loosely based around the following paper: "Video Google: A Text Retrieval Approach to Object Matching in Videos", by J. Sivic and A. Zisserman, published in ICCV 2003, which can be found <a href="http://www.robots.ox.ac.uk/%7Evgg/publications/papers/sivic03.pdf">here</a>.
<br><br>
<img src="video_google_1.png"/>
<br>
<b>Data and provided code</b>
<br><br>
At <a href="http://people.cs.pitt.edu/~kovashka/cs1699/hw3_files/">this link</a>, courtesy of Kristen Grauman, you can find precomputed SIFT features (<font face="courier new">sift.zip</font>) for frames in a "Friends" episode, as well as the associated frames as images (<font face="courier new">frames.zip</font>), and provided starter code (<font face="courier new">code.zip</font>).
The data takes up about 5.5G, so if possible, do not copy it but point to it directly in your code (<font face="courier new">//afs/cs.pitt.edu/courses/cs1699/sift/, //afs/cs.pitt.edu/courses/cs1699/frames/</font>).
<br><br>
Each <font face="courier new">.mat</font> file in <font face="courier new">sift.zip</font> corresponds to a single image, and contains
the following variables, where <i>n</i> is the number of detected SIFT features in that image:
<br><br>
<table border=0>
<tr><td width=150><u>Variable</u></td><td width=120><u>Size + type</u></td><td><u>Description</u></td></tr>
<tr><td><font face="courier new">descriptors</font></td><td><i>n</i>x128 double</td><td>the SIFT vectors as rows</td></tr>
<tr><td><font face="courier new">imname</font></td><td>1x57 char</td><td>name of the image file that goes with this data</td></tr>
<tr><td><font face="courier new">numfeats</font></td><td>1x1 double</td><td>number of detected features</td></tr>
<tr><td><font face="courier new">orients</font></td><td><i>n</i>x1 double</td><td>the orientations of the patches</td></tr>
<tr><td><font face="courier new">positions</font></td><td><i>n</i>x2 double</td><td>the positions of the patch centers</td></tr>
<tr><td><font face="courier new">scales</font></td><td><i>n</i>x1 double</td><td>the scales of the patches</td></tr>
</table>
<br>

The provided code includes the following. Feel free to copy relevant parts of the provided code in the functions you have to implement. 
<ul>
<li>
<font face="courier new">loadDataExample.m:</font> <b>Read and run this first</b> and make sure you understand the data format. Note that you'll have to modify the paths for the frames and SIFT files. It
is a script that shows a loop of data files, and how to access each descriptor. It also
shows how to use some of the other functions below.
</li>
<li>
<font face="courier new">displaySIFTPatches.m:</font> Given SIFT descriptor info, it draws the patches on top of an
image.
</li>
<li>
<font face="courier new">getPatchFromSIFTParameters.m:</font> Given SIFT descriptor info, it extracts the image
patch itself and returns as a single image.
</li>
<li>
<font face="courier new">selectRegion.m:</font> Given an image and list of feature positions, it allows a user to draw a
polygon showing a region of interest, and then returns the indices within the list of
positions that fell within the polygon.
</li>
<li>
<font face="courier new">dist2.m:</font> A fast implementation of computing pairwise distances between two matrices
for which each row is a data point.
</li>
<li>
<font face="courier new">kmeansML.m:</font> A fast k-means implementation that takes the data points as <i>columns</i>.
</li>
</ul>
You are not required to use any of these functions, but you will probably find them helpful. 
<br>
<br>
<b>What to implement and discuss in your write-up</b>
<br><br>
Write one script for each of the following (along with any helper functions you find useful), and in
your writeup report on the results, explain, and show images where appropriate. Implement the functionality in the order given below.
<br>
<ol>
<li>[5 pts] <font face="courier new">function [inds] = matchRawDescriptors(d1, d2)</font> that computes nearest raw SIFT descriptors. <font face="courier new">d1, d2</font> are <i>M</i>x128 and <i>N</i>x128 are sets of descriptors for two images respectively, where <i>M, N</i> are the number of keypoints detected in the first and second image. <font face="courier new">inds</font> should contain those indices over the full set of <i>N</i> descriptors in the second image that match some descriptor in the first image. 
<ul>
<li>Use the Euclidean distance between SIFT descriptors to determine which are nearest among two images' descriptors. That is,
"match" each feature from the first image to its nearest neighbor in the second image, and store as many indices into the second image's descriptor set as you have features in the selected region of the first image.</li>
<li>Do not quantize to visual words at this step.</li>
<li>To find the minimum values and indices in a matrix X along the <i>i</i>-th dimension, use <font face="courier new">min(X, [], i)</font>.</li> 
</ul>
</li>
<br>
<li>[10 pts] A script <font face="courier new">rawDescriptorMatches.m</font> that allows a user to select a region of interest in one frame (see
provided <font face="courier new">selectRegion</font> function), and then match descriptors in that region to
descriptors in the second image based on Euclidean distance in SIFT space (use the <font face="courier new">matchRawDescriptors</font> you wrote above). 
<ul>
<li>Run your code on the two images and associated
features in the provided file <font face="courier new">twoFrameData.mat</font> (in the provided code zip file; run <font face="courier new">load('twoFrameData');</font>) to demonstrate. </li>
<li>Select a region of interest (a polygon) in the first image, and display the matched features in the second image, something like the below example. Include in your write-up a screenshot of the selected region, as well as the retrieved matched features, for <i>three</i> different selected image regions.</li> 
<li>See provided helper function <font face="courier new">displaySIFTPatches</font>.</li>
</ul>
</li>
</li>
<img src="video_google_2.png"/>
<br><br>
<li>
[15 pts] A script <font face="courier new">visualizeVocabulary.m</font> to visualize a vocabulary. 
<ul>
<li>First, load some SIFT descriptors and compute a k-means clustering using those. You want to cluster a large, representative random sample of SIFT
descriptors from some of the frames. (If you use a sample from just a single part of the episode, you will not get good results because your sample is not representative.) Note that you may run out of memory if you use all descriptors. Select a random sample of frames, and a random sample of features within each frame (see <font face="courier new">randperm</font>). Sample 300 frames and at most 100 (<font face="courier new">1:min(100, length(...))</font>) features per frame.</li>
<li>Then run <font face="courier new">kmeansML</font> to get your cluster memberships and means. Let the <i>k</i> centers be the visual words. The value of <i>k</i> is a free parameter; for this data something like <i>k</i>=1500
should work, but feel free to play with this parameter. See the provided <font face="courier new">kmeansML.m</font> code.</li>
<li>Choose two words that are distinct enough. (Hint: use the distances between them to pick those words automatically, but first restrict the choice of clusters to show to those that have at least 25 patches.) </li>
<li>Display example image patches associated with the two visual words you've chosen. The goal is to show what the different words are capturing, and display enough patch examples so
the word content is evident (e.g., say 25 patches per word displayed), similar to slide 59 <a href="http://people.cs.pitt.edu/~kovashka/cs1699/vision_10_matching_retrieval.pdf">here</a>. </li>
<li>See the provided helper function <font face="courier new">getPatchFromSIFTParameters</font>.</li>
<li>Remember to keep track of the parent image for each feature.</li>
<li>Save the cluster memberships and means into a file, using <font face="courier new">save('centers.mat', 'membership', 'means');</font></li>
<li>In your writeup, include your two visualized clusters, and explain what you see.</li>
</ul>
<br>
</li>
<li>[5 pts] <font face="courier new">function [bow] = computeBOWRepr(descriptors, means)</font> to compute a Bag-of-Words (BOW) representation of an image or image region (polygon). <font face="courier new">bow</font> is a normalized bag-of-words histogram. <font face="courier new">descriptors</font> is the <i>M</i>x128 set of descriptors for the image or image region, and <font face="courier new">means</font> is the <i>k</i>x128 set of cluster means.
<ul>
<li>
Map a raw SIFT descriptor to its visual word. The raw descriptor is assigned to the
nearest visual word. See the provided <font face="courier new">dist2.m</font> code for fast distance computations.
</li>
<li>
Map an image or region's features into its bag-of-words histogram. The histogram for image
<i>I<sub>j</sub></i> is a <i>k</i>-dimensional vector: <i>F(I<sub>j</sub>) = [&nbsp;freq<sub>1, j</sub> &nbsp;&nbsp; freq<sub>2, j</sub> &nbsp;&nbsp; ... &nbsp;&nbsp; freq<sub>k, j</sub>&nbsp;]</i>, where each entry
<i>freq<sub>i, j</sub></i> counts the number of occurrences of the <i>i</i>-th visual word in that image, and <i>k</i> is
the number of total words in the vocabulary. In other words, a single image's or image region's list of <i>M</i>
SIFT descriptors yields a <i>k</i>-dimensional bag of words histogram. </li>
<li>Matlab's <font face="courier new">histc</font> is a useful function. </li>
<li>Load the cluster means from your saved file.</li>
<li>You don't have to implement TF-IDF weighting.</li>
<!--Weight it with tf-idf.-->
</li>
</ul>
</li>
<br>
<li>[5 pts] <font face="courier new">function [sim] = compareSimilarity(bow1, bow2)</font> to compare the similarity score for two bag-of-words histograms using the normalized scalar product, as in slide 69 <a href="http://people.cs.pitt.edu/~kovashka/cs1699/vision_10_matching_retrieval.pdf">here</a>.
</li>
<br>
<li>
[10 pts] A script <font face="courier new">fullFrameQueries.m</font> to process full frame queries.
<ul>
<li>This part is similar to the last sub-part of HW2, but using a better similarity metric and a lot more images.</li>
<li>Choose 3 different frames from the entire video dataset to serve as queries.</li>
<li>Display the <i>M</i>=5 most similar frames to each of these queries (in rank order) based on the
normalized scalar product between their bag of words histograms. Sort the similarity scores between a query histogram and the histograms associated
with the rest of the images in the video. Pull up the images associated with the <i>M</i> most
similar examples. See Matlab's <font face="courier new">sort</font> function.
</li>
<li>Load the cluster means from your saved file.</li>
<li>For debugging, just load a small number of frames (and pick your query among those), compute their BOW, and leave the other frames' BOWs initialized to all zeros.</li>
<li>In your write-up, include each query along with its most similar 3 images, and explain the results.</li>
</ul>
</li>
<br>
<li>
[15 pts] A script <font face="courier new">regionQueries.m</font> to process region queries.
<ul>
<li>Select your favorite query regions from each of 4 frames (which
may be different than those used above) to demonstrate the top 3 retrieved frames when only
a portion of the SIFT descriptors are used to form a bag of words.</li>
<li>Form a query from a region within a frame. Select a polygonal region interactively
with the mouse, and compute a bag of words histogram from only the SIFT descriptors
that fall within that region.</li>
<li>Try to include example(s) where the same object appears amidst
different objects or backgrounds, and also include a failure case.</li>
<li>Don't retrieve results from the same image as the query region.</li>
<li>See the provided <font face="courier new">selectRegion.m</font> code, and re-use some of your code from the part above.</li>
<li>Run a <font face="courier new">close all</font>, or open a new figure, before running the region selection.</li>
<li>In your write-up, include each of the 4 queries (take a screenshot of the image with selected region) along with its most similar 3 images, and explain the results, including possible reasons for the failure cases. </li>
</ul>
</li>
</ol>

<br><b>Acknowledgement:</b> Both parts of this homework are adapted from assignments by Kristen Grauman.
<br><br>

</body>
</html>