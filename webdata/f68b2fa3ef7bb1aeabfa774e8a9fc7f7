<html>
<head>
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</head>
<bod bgcolor="#FFFBEC" text="#000000" link="#802000" alink="#FF2020" vlink="#606060"y>
<h1>The traveling yelper</h1>
<ol>
  <li>Today I'll be talking about about a particular problem for
  performing NLP on the Yelp review dataset.  Your final project in
  this class will need to work on this dataset, though not necessarily
  address this problem.
  <li>Online reviews are now commonplace
    <ol>
      <li>Not all reviews are useful to all people.  People don't all
      agree in all things and we want reviews from people we might
      actually agree with.
      <li>The classic approach to this is the <i>trusted reviewer</i>.
      In a pre-computational setting, a user learned which reviewers
      they agreed with, and only went to those reviewers.
      Computationally, we could tell a piece of software (<i>Stumble,
      Flipbook, yelp</i> etc.) about what reviews we liked, and it remembers who the "good" reviewers are.
      <li>This works OK for things like movies, where every (major) reviewer
      rates every (major) movie, but starts to break down when the
      number of items to be reviewed is too large for a small set of
      trusted reviewers.
      <li>We want to be able to recommend either reviewers or items directly to the user.
    </ol>
  <li>Collaborative Filtering
    <ol>
      <li>The most common solution to this problem is known as Collaborative Filtering (not a great name).
      <li>The basic insight is that individuals who have similar
      opinions in one area are likely to have similar opinions in
      others.  
      <li>Just compare a person to other people based on their
      behavior (usually ratings), and come up with a measure of
      similarity.
      <li>Two modes: recommendations and predicted ratings.
      <li>For recommendations, find the most similar person in the
      entire set, find something else that person likes, and recommend
      it to you.
      <li>For predicted ratings for an item, find the most similar
      person from the set of people who rated this item, and use their
      score.
      <li>[Item based], find similarity between items by looking at
      people who rated both items and seeing if the ratings are
      similar.
      <li>Can this be noisy?  Sure, lets smooth it.  Average out a few of the closest people.  Weight scores by closeness.
      <li>But what is our measurement metric?  The most common ones are all related to the linear correlation of the two people.
      <li>Take each rating made by person x and compare to the rating
      by person y on the same item.  We can plot that sort of thing
      like so:<br>
	<img src="BinomCorr84.png"><br>
      <li>We can use the standard statistical tests to find the
      correlation between the two variables.  In this case it was
      about 0.84, pretty high, which makes sense looking at the data.
      <li>What is the actual equation?<br>
\[\begin{align}
\bar{x}&\text{ is the mean of the x values}\\
I &\text{ is the set of items}\\
\rho =& \frac{\sum_{i\in I}(x_i - \bar{x})(y_i - \bar{y})}
{\sqrt{\sum_{i \in I}(x_i - \bar{x})^2}\sqrt{\sum_{i \in I}(y_i - \bar{y})^2}}
\end{align} \]
      <li>This is just the covariance over the product of the standard deviations.
      <li>What about missing values?  As in if x reviews some things y doesn't and vice versa?
      <li>Easiest thing: Limit set $I$ to things both users rated.
      But if set is small, there are a number of clever hacks, like,
      predict a rating by using a users average, or by using a similar
      item (measured as having been rated the same by the same
      people... etc).
      <li>There are hundreds of these little tweaks, many of the proprietary secrets. (Amazon is notoriously closed about this, Netflix is remarkably open)
      <li>Model-based CF - use machine learning techniques to model other
      <li>Naive Bayes. Assume We have a bunch of ratings for items that I have also rated.  We can ask, what is the probability that I would rank it, say 2, given that user 1 gave it 2, user 2 gave it 3, etc.
\[\begin{align}
\arg\max_i P(C_i|U_1,U_2,\cdots,U_m) =& \arg\max_i\frac{P(U_1,U_2,\cdots,U_m | C_i)P(C_i)}{P(U_1,U_2,\cdots,U_m)}\\
=& \arg\max_i P(U_1|C_i)P(U_2|C_i)\cdots P(U_m|C_i)P(C_i)\\
=& \arg\max_i P(C_i)\Pi_j P(U_j|C_i)
\end{align}\]
      <li>Clustering.  Group users or items into clusters (k-means,
      etc.)  Essentially the basic CF technique is a form of
      clustering where the similar users form a cluster around the
      user in question.  I mention this because there are other
      clustering techniques that are more efficient.  For instance, if
      we store k-means, we only have to compare a user to the k means,
      and then to the other users in that cluster.
      <li>Others: linear regression, Markov-Decision-Process, heck
      throw a neural net at it and hope it sticks.  Name a ML
      technique and it has been applied here.
      <li>Content-based 
	<ol>
	  <li>Include other stuff: users zip code, gender, occupation.
	  <li>Or look atthe content of the item itself.  Pandora Radio.  Music Genome Project
	  <li>CONTENT OF THE REVIEW!  hey! NLP after all!
	  <li>One of the first
	  was <a href="http://www.cs.utexas.edu/~ml/papers/libra-sigir-wkshp-99.pdf">Mooney
	  & Roy (1999)</a>.  
	    <ol>
	      <li>Built a bag of words representation of a processed
		version of the amazon web-page, including title,
		author, published reviews, and user reviews.
	      <li>User rates a set of "training" books.
	      <li>They built naive Bayes classifiers (one for each
	      rating) trained on the BoWs.  Theres more magic than
	      that in the paper,
	      <li>One issue- data sparsity.  
	    </ol>
	  <li>Lots of work in <a href="http://lmgtfy.com/?q=content+boosted+collaborative+filtering">"Content boosted CF".</a>
	  <li>A more recent one (we'll talk about below)
	  is <a href="http://www.yelp.com/html/pdf/YelpDatasetChallengeWinner_PersonalizingRatings.pdf">Linshi</a>
	  that we'll talk more about later.  Now I want to talk about the problem....
	</ol>
      <li>All of these techniques need a lot of data.
    </ol>
  <li>Traveling Yelper
    <ol>
      <li>What if there's no overlap?  If I travel between small and
      medium-sized cities and I look at restaurants on yelp, there's a
      good chance there's no person in my new locations has reviewed a
      single restaurant that I have reviewed.  All of the techniques
      above build on relations between overlap of review scores.
      <li>What we would like to do if figure out who the similar
      people are based not on their scores, but <i>what they say in
      their review</i>.  The intuition is that the sort of things I
      care about are also going to be the sort of things people who
      like the items I like, care about.
      <li>What does this person care about?<br>
	<img src="meat.png">
      <li>What does this person care about?<br>
	<img src="Vallozi.png">
      <li>We can tell a lot about these reviewers from what they say above.  Are they similar to you? Dissimilar?
      <li>Proposal: perform CF-like process by finding reviews who
      care about the same kinds of things you do, based on the text of
      your reviews and the text of the target reviews.
    </ol>
  <li>Issues and Techniques
    <ol>
      <li>Will Mooney & Roy work?  Maybe, but data is a problem.
      <li>This involves topic modeling (what is some text about) but it is not the same as topic modeling: <br>
	<img src="vallozi2.png"><br>
	This review is also about price, but what this person cares about is very different in regards to price.
      <li>You'll note my change in language above.  We don't
      necessarily have to match to other reviewers, we can look at
      reviews.  A reviewer may be different from me, but have a
      particularly "like me" review.
      <li>Two related topics to discuss, sentiment analysis and topic modeling.
      <li>Sentiment Analysis
	<ol>
	  <li>What is the <i>attitude</i> of some text toward some object?   What is the target/aspect, polarity, intensity?
	  <li>Basic algorithm: Tokenize document; Extract features;
	  Classify
	  (cf. <a href="http://www.cs.cornell.edu/home/llee/omsa/omsa.pdf">Pang,
	  Lee '08</a>)
	  <li>Tokenization: get rid of HTML, hashtags, normalize text
	  (preserve all-caps? use emoticons?)  What to do with "not"?
	  add NOT_ to all words after not until punctuation.  "the
	  place was not very busy, yet the service was horribly slow."
	  becomes "the place was NOT_very NOT_busy, yet the service
	  was horribly slow."  Boolean instead of count?
	  <li>Break out Naive Bayes again:<br>
\[\begin{align}
c =& \arg\max_i P(C_i)\Pi_{j \in \text{Positions}}P(w_j|c_i)\\
\hat{P}(W|C) =& \frac{\text{count}(w,c)+1}{\text{count}(c) + |V|}\\
P(C_i) =& \frac{|docs_i|}{|docs|}
\end{align}\]
	  <li>Other classifiers work fine too.
	  <li>Create a sentiment lexicon to help with classification.
	  Some already exist, such
	  as <a href="http://mpqa.cs.pitt.edu/"> MPQA</a> developed
	  here at Pitt
	  (<a href="http://people.cs.pitt.edu/~wiebe/pubs/papers/twilsonDissertation2008.pdf">Wilson 08</a>).
	  <li>These lexicons can also be learned (good for specialized
	  domains), using review scores, words they're used with, or
	  thesauruses.
	  <li>In order to do the fine grained analysis I'm suggesting, we need to also know the target.  There have been various attempts to figure these out automatically, (<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.2378&rep=rep1&type=pdf">Hu and Liu 04</a>).
	  <li>But for many domains, we know the target sets already.
	    Restaurants are food, decor, ambiance, etc.  Can train a
	    sentence classifier for that, then label each sentence
	    with a sentiment and target, then collect in some way to
	    form the model.
	  <li>Incomplete, but getting there.
	</ol>
      <li>Topic Modeling and Latent Dirichlet allocation.
	<ol>
	  <li>LDA is a method for automatically identifying topics of documents via a an unsupervised model.  See <a href="http://www.cs.princeton.edu/~blei/papers/Blei2012.pdf">(Blei 12)</a>
	  <li>Assume a <i>Topic</i> is a distribution over a fixed vocabulary.  That is if a document is of a topic, when it was written, words were drawn randomly from a hat according to those topic distributions.
	  <li>Assume that when a document was written the author first randomly selected a distribution over topics (Documents are usually about multiple things at once.
	  <li>For each word in the document, the author randomly
	  chooses a topic for that word from the above distribution,
	  and then uses the topic distribution to randomly select a word.
	  <li>So all we have to do is, looking at the actual
	  documents, go backward and figure out what those
	  distributions are.<br>
	    <img src="lda.png" alt="">
	  <li>We want to find the posterior:<br>
\[\begin{align}
P(\beta,\theta,z|w) =&\frac{P(\beta,\theta,z,w)}{P(w)}
\end{align}\]<br>

	    The numerator can be computed directly, and the
	    denominator can be estimated through various techniques,
	    such as <a href="http://en.wikipedia.org/wiki/Gibbs_sampling">Gibb's sampling.</a>
	  <li>So, what does identifying the topic do to help with the
	  problem?  One approach is to learn the hidden topics, and
	  use that as input to another ML algorithm <a href="https://www.ideals.illinois.edu/bitstream/handle/2142/48832/Huang-iConference2014-SocialMediaExpo.pdf?sequence=2">(Huang et al., 2014)</a>
	  <li>What if we expand the LDA to include ratings?<br>
	    <img src="lda+r.png" alt=""><br>
	    <a href="http://www.yelp.com/html/pdf/YelpDatasetChallengeWinner_PersonalizingRatings.pdf">(Linshi 14)</a>
	  <li>Empirically, while the distributions of topics do appear
	  to be different based on the review, when performing the
	  full analysis, no topics had any sentiment elements.
	  <li>Used lexicon to add codewords to text.  After every
	  positive word, add GOODREVIEW and after every negative add
	  BADREVIEW.  The claim is that the frequency of co-occurrence
	  between the code words and the topic words will cause the
	  sentiment to leak into the topics themselves.a
	</ol>
      <li>Still missing?
	<ol>
	  <li>Don't directly model what you actually <i>care</i>
	    about.  It could discuss a topic without it mattering:
	    "the ambiance sucks, but what a taco!" or "The ambiance
	    sucked, but what bothered me was the lousy taco".  Or see
	    the second Vallozzi review above.  Maybe a more explicit
	    topic + sentiment model would help.
	  <li>Work on the review level?
	  <li>A more principled approach.  Codewords force a
	  particular behavior out of the algorithm, but without some
	  principle behind it, there's no reason to believe it will
	  scale.
	  <li>Too small a lexicon.
	  <li>Decent evaluation.  Friends? Useful-funny-cool?
	  <li>A direct comparison of all the techniques.
	</ol>
      <li>The data?  <a href="http://www.yelp.com/dataset_challenge">Yelp Dataset Challenge</a>.  Why?  Good data, covering good problem.  Available in json format, and there's prize money!
    </ol>
</ol>
</body>
